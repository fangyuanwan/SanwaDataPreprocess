"""
3Bæ¨¡å‹æ•°æ®æ¸…ç†ç®¡é“ - å®Œæ•´è‡ªåŠ¨åŒ–æµç¨‹
3B Model Data Cleaning Pipeline - Full Automation

æµç¨‹ Pipeline:
1. æ•°æ®éªŒè¯å’Œæ¸…ç†ï¼ˆåŸºäºå¢å¼ºé€»è¾‘ï¼‰
2. å¼‚å¸¸æ£€æµ‹å’Œæ ‡è®°
3. ä½¿ç”¨3Bæ¨¡å‹ä¿®æ­£å¼‚å¸¸å€¼
4. åˆå¹¶ä¿®æ­£ç»“æœ
5. é‡æ–°æ ‡è®°å’Œåˆ†ç±»

è¾“å…¥ Input: Stage 1 OCRç»“æœ
è¾“å‡º Output: Stage 4 å·²æ ‡è®°çš„æ¸…ç†æ•°æ®
"""

import pandas as pd
import numpy as np
import os
import sys
import json
import glob
import shutil
import re
import cv2
import ollama
from pathlib import Path
from datetime import datetime
import concurrent.futures
import threading
from collections import defaultdict

# å¯¼å…¥é…ç½®
from config_pipeline import *

print_lock = threading.Lock()

# ================= é˜¶æ®µ1: æ•°æ®éªŒè¯å’Œæ¸…ç† =================
class DataValidator:
    """æ•°æ®éªŒè¯å™¨ - æ£€æµ‹å¼‚å¸¸å€¼"""
    
    def __init__(self, max_decimals=3, outlier_threshold=5.0):
        self.max_decimals = max_decimals
        self.outlier_threshold = outlier_threshold
    
    def validate_value(self, val, data_type):
        """
        éªŒè¯å•ä¸ªå€¼
        è¿”å›: (is_valid, clean_val, reason)
        """
        val_str = str(val).strip()
        if pd.isna(val) or val_str == '' or val_str.lower() == 'nan':
            return False, val, "Empty/NaN"
        
        if data_type == 'STATUS':
            val_upper = val_str.upper()
            # åˆ†ç±»è§„åˆ™ï¼šåªæœ‰OKæˆ–NGä¸¤ç§è¾“å‡º
            # ä»¥Oå¼€å¤´ â†’ OK (O, OH, OK, 0)
            # ä»¥Nå¼€å¤´ â†’ NG (N, NG, NH, NO, NaN)
            
            if val_upper.startswith('O') or val_upper == '0' or 'OK' in val_upper:
                return True, 'OK', None
            if val_upper.startswith('N'):
                # N, NG, NH, NO, NaN éƒ½åˆ†ç±»ä¸ºNG
                return True, 'NG', None
            if val_upper == 'K':
                return True, 'OK', None
            if val_upper == 'G':
                return True, 'NG', None
            # ç©ºç™½æˆ–æ— æ³•è¯†åˆ« â†’ æ ‡è®°ä¸ºéœ€è¦æ£€æŸ¥
            if val_upper in ['', 'NAN', 'NA', 'NULL', 'NONE']:
                return False, val, "Empty/Invalid Status - needs review"
            # å…¶ä»–æƒ…å†µæ ‡è®°ä¸ºéœ€è¦æ£€æŸ¥
            return False, val, "Unknown Status - needs review"
        
        elif data_type == 'INTEGER':
            clean_val = re.sub(r'[^\d-]', '', val_str)
            if re.match(r'^-?\d+$', clean_val):
                return True, int(clean_val), None
            return False, val, "Not an Integer"
        
        elif data_type == 'FLOAT':
            if re.match(r'^-?\d+(\.\d+)?$', val_str):
                if '.' in val_str and len(val_str.split('.')[1]) > self.max_decimals:
                    return False, val, f"Suspicious Pattern (>{self.max_decimals} decimals)"
                try:
                    return True, float(val_str), None
                except:
                    pass
            return False, val, "Invalid Float"
        
        elif data_type == 'TIME':
            if re.match(r'^\d{1,2}:\d{2}:\d{2}$', val_str):
                return True, val_str, None
            return False, val, "Invalid Time"
        
        return False, val, "Unknown Type"
    
    def detect_outliers(self, series, data_type):
        """ç»Ÿè®¡å¼‚å¸¸å€¼æ£€æµ‹"""
        if data_type not in ['FLOAT', 'INTEGER']:
            return []
        
        nums = pd.to_numeric(series, errors='coerce').dropna()
        if len(nums) < 5 or nums.median() == 0:
            return []
        
        median = nums.median()
        outlier_indices = []
        
        for idx, val in series.items():
            try:
                ratio = float(val) / median
                if ratio > self.outlier_threshold or ratio < (1.0 / self.outlier_threshold):
                    outlier_indices.append(idx)
            except:
                pass
        
        return outlier_indices

class Stage1_DataCleaning:
    """é˜¶æ®µ1: æ•°æ®æ¸…ç†å’Œå¼‚å¸¸æ£€æµ‹"""
    
    def __init__(self, input_dir, output_dir, crops_base):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.crops_base = Path(crops_base)
        self.validator = DataValidator(MAX_DECIMALS, OUTLIER_THRESHOLD)
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def get_config_for_file(self, df):
        """è¯†åˆ«æ–‡ä»¶å¯¹åº”çš„ROIé…ç½®"""
        for config in ROI_CONFIGS:
            if config['Trigger_Col'] in df.columns:
                return config
        return None
    
    def copy_crop_for_review(self, csv_base_name, filename, roi_id, dest_folder):
        """å¤åˆ¶å¼‚å¸¸è£å‰ªå›¾åƒä¾›äººå·¥æ£€æŸ¥"""
        try:
            folder_name = os.path.splitext(filename)[0]
            
            # æœç´¢è·¯å¾„
            potential_paths = [
                self.crops_base / csv_base_name / folder_name / f"{roi_id}.jpg",
                self.crops_base / csv_base_name / folder_name / f"{roi_id}.png",
                self.crops_base / folder_name / f"{roi_id}.jpg",
                self.crops_base / folder_name / f"{roi_id}.png",
            ]
            
            src_file = None
            for p in potential_paths:
                if p.exists():
                    src_file = p
                    break
            
            if not src_file:
                return False
            
            target_folder = dest_folder / folder_name
            target_folder.mkdir(parents=True, exist_ok=True)
            shutil.copy(src_file, target_folder / src_file.name)
            return True
        except:
            return False
    
    def process_single_csv(self, csv_path):
        """å¤„ç†å•ä¸ªCSVæ–‡ä»¶"""
        filename = csv_path.name
        base_name = csv_path.stem
        
        print(f"\nğŸ“„ Processing: {filename}...")
        
        try:
            df = pd.read_csv(csv_path)
        except Exception as e:
            print(f"  âŒ Error reading CSV: {e}")
            return
        
        config = self.get_config_for_file(df)
        if not config:
            print(f"  âš ï¸  Skipped: Unknown format")
            return
        
        roi_map = config['Columns']
        
        # æ’åº
        if 'Filename' in df.columns:
            df.sort_values(by='Filename', inplace=True)
        
        df_clean = df.copy()
        abnormal_records = []
        
        # é˜¶æ®µ1: é€è¡ŒéªŒè¯
        print(f"  ğŸ” Validating {len(df)} rows...")
        for idx, row in df.iterrows():
            for roi_col, dtype in roi_map.items():
                if roi_col in df.columns:
                    val = row[roi_col]
                    is_valid, clean_val, reason = self.validator.validate_value(val, dtype)
                    
                    if is_valid:
                        df_clean.at[idx, roi_col] = clean_val
                    else:
                        abnormal_records.append({
                            'Filename': row.get('Filename', 'Unknown'),
                            'Timestamp': row.get('ROI_52', ''),
                            'ROI_ID': roi_col,
                            'Value': val,
                            'Reason': reason
                        })
        
        # é˜¶æ®µ2: ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹
        print(f"  ğŸ“Š Detecting statistical outliers...")
        for roi_col, dtype in roi_map.items():
            if roi_col in df_clean.columns:
                outliers = self.validator.detect_outliers(df_clean[roi_col], dtype)
                for idx in outliers:
                    abnormal_records.append({
                        'Filename': df_clean.at[idx, 'Filename'],
                        'Timestamp': df_clean.at[idx, 'ROI_52'] if 'ROI_52' in df_clean.columns else '',
                        'ROI_ID': roi_col,
                        'Value': df_clean.at[idx, roi_col],
                        'Reason': "Statistical Outlier (Likely Missing Decimal)"
                    })
        
        # ä¿å­˜ç»“æœ
        df_clean.to_csv(self.output_dir / f"{base_name}_Cleaned.csv", index=False)
        
        if abnormal_records:
            df_abn = pd.DataFrame(abnormal_records).drop_duplicates()
            df_abn.to_csv(self.output_dir / f"{base_name}_Abnormal_Log.csv", index=False)
            
            # å¤åˆ¶å¼‚å¸¸å›¾åƒ
            crop_dest = ABNORMAL_CROPS_BASE / base_name
            crop_dest.mkdir(parents=True, exist_ok=True)
            
            count = sum(1 for _, rec in df_abn.iterrows() 
                       if self.copy_crop_for_review(base_name, rec['Filename'], 
                                                   rec['ROI_ID'], crop_dest))
            
            print(f"  âš ï¸  Found {len(df_abn)} issues. Copied {count} images.")
        else:
            print(f"  âœ… No issues found.")
        
        print(f"  ğŸ’¾ Saved: {base_name}_Cleaned.csv")
    
    def run(self):
        """è¿è¡Œæ¸…ç†æµç¨‹"""
        print("\n" + "="*60)
        print("STAGE 1: Data Validation and Cleaning")
        print("="*60)
        
        csv_files = list(self.input_dir.glob("**/*.csv"))
        csv_files = [f for f in csv_files if not any(x in f.name for x in ['_Cleaned', '_Log', '_Abnormal'])]
        
        if not csv_files:
            print("âŒ No CSV files found in input directory")
            return
        
        print(f"Found {len(csv_files)} CSV files\n")
        
        for csv_file in csv_files:
            self.process_single_csv(csv_file)
        
        print("\nâœ… Stage 1 Complete")

# ================= é˜¶æ®µ2: 3Bæ¨¡å‹å¼‚å¸¸ä¿®æ­£ =================
class Stage2_3BCorrection:
    """é˜¶æ®µ2: ä½¿ç”¨3Bæ¨¡å‹ä¿®æ­£å¼‚å¸¸å€¼"""
    
    def __init__(self, cleaned_dir, abnormal_logs_dir, crops_base, output_dir):
        self.cleaned_dir = Path(cleaned_dir)
        self.abnormal_logs_dir = Path(abnormal_logs_dir)
        self.crops_base = Path(crops_base)
        self.output_dir = Path(output_dir)
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def calculate_roi_medians(self, csv_path):
        """
        ä»CSVè®¡ç®—æ¯ä¸ªROIçš„medianå€¼
        è¿”å›: {roi_id: median_value}
        """
        try:
            df = pd.read_csv(csv_path)
            roi_medians = {}
            
            print(f"  ğŸ“Š Calculating medians from {len(df)} rows...")
            
            for col in df.columns:
                if not col.startswith('ROI_'):
                    continue
                
                roi_type = get_roi_type(col)
                
                # åªå¯¹æ•°å€¼ç±»å‹è®¡ç®—median
                if roi_type in ['INTEGER', 'FLOAT']:
                    try:
                        # è½¬æ¢ä¸ºæ•°å€¼ï¼Œå¿½ç•¥é”™è¯¯
                        vals = pd.to_numeric(df[col], errors='coerce').dropna()
                        # è¿‡æ»¤æ‰0å€¼ï¼ˆå¯èƒ½æ˜¯ç¼ºé™·ï¼‰
                        vals = vals[vals > 0]
                        
                        if len(vals) >= 5:  # è‡³å°‘5ä¸ªæœ‰æ•ˆæ ·æœ¬
                            roi_medians[col] = vals.median()
                            print(f"    âœ“ {col}: Median={roi_medians[col]:.3f} (from {len(vals)} samples)")
                    except Exception as e:
                        print(f"    âš ï¸  {col}: Could not calculate median - {e}")
                
                elif roi_type == 'STATUS':
                    # å¯¹äºSTATUSï¼Œæ‰¾å‡ºæœ€å¸¸è§çš„å€¼
                    try:
                        value_counts = df[col].value_counts()
                        if not value_counts.empty:
                            roi_medians[col] = value_counts.index[0]  # æœ€å¸¸è§çš„å€¼
                            print(f"    âœ“ {col}: Most common={roi_medians[col]}")
                    except Exception as e:
                        print(f"    âš ï¸  {col}: Could not find mode - {e}")
            
            print(f"  ğŸ“Š Calculated medians for {len(roi_medians)} ROI fields")
            return roi_medians
            
        except Exception as e:
            print(f"  âŒ Error calculating medians: {e}")
            return {}
    
    def run_3b_inference(self, image_path, roi_id, median_val, ocr_value):
        """ä½¿ç”¨3Bæ¨¡å‹é‡æ–°è¯†åˆ«"""
        try:
            prompt = get_prompt(roi_id, 'correction', ocr_value, median_val)
            
            response = ollama.chat(
                model=OLLAMA_MODEL_3B,
                messages=[{
                    'role': 'user',
                    'content': prompt,
                    'images': [str(image_path)]
                }],
                options={'temperature': 0.0, 'num_predict': 30}
            )
            
            text = response['message']['content']
            
            # æ¸…ç†è¾“å‡º
            text = re.sub(r'<[^>]+>', '', text).replace('```', '').replace('`', '').strip()
            text = re.sub(r'^(Output:|Result:)', '', text, flags=re.IGNORECASE).strip()
            
            # åå¤„ç†ï¼šä¿®å¤å¸¸è§æ ¼å¼é”™è¯¯
            roi_type = get_roi_type(roi_id)
            text = self.post_process_number(text, roi_type, median_val)
            
            return text if text else "ERROR"
            
        except Exception as e:
            print(f"  [3B Error] {e}")
            return "ERROR"
    
    def post_process_number(self, text, roi_type, median_val):
        """
        åå¤„ç†æ•°å­—è¾“å‡º - åªåšåŸºæœ¬æ¸…ç†ï¼Œä¸è‡ªåŠ¨ä¿®å¤é‡å¤æ¨¡å¼
        - æ¸…ç†markdownæ ‡è®°
        - æˆªæ–­å°æ•°ä½æ•°åˆ°3ä½
        - æ£€æµ‹é—®é¢˜å¹¶è­¦å‘Šï¼ˆä¸è‡ªåŠ¨ä¿®å¤ï¼‰
        """
        if not text or text in ["ERROR", "NA", "Image Not Found"]:
            return text
        
        original = text
        
        if roi_type == 'FLOAT':
            # 1. æ£€æµ‹å¤šå°æ•°ç‚¹ (e.g., '5.7.726') - åªè­¦å‘Šï¼Œä¸è‡ªåŠ¨ä¿®å¤
            decimal_count = text.count('.')
            if decimal_count > 1:
                print(f"    âš ï¸ [Warning] Multiple decimals detected: '{text}' - keeping as-is for review")
            
            # 2. æ£€æµ‹å¯èƒ½çš„é‡å¤æ¨¡å¼ - åªè­¦å‘Šï¼Œä¸è‡ªåŠ¨ä¿®å¤
            repeat_match = re.match(r'^(-?\d+\.\d{1,3})\1+', text)
            if repeat_match:
                print(f"    âš ï¸ [Warning] Possible repeat pattern: '{text}' - keeping as-is for review")
            
            # 3. åªæˆªæ–­è¿‡é•¿çš„å°æ•°ä½æ•°ï¼ˆè¿™æ˜¯æ ¼å¼æ ‡å‡†åŒ–ï¼Œä¸æ˜¯ä¿®å¤ï¼‰
            if '.' in text:
                parts = text.split('.')
                if len(parts) == 2 and len(parts[1]) > 3:
                    text = f"{parts[0]}.{parts[1][:3]}"
                    if text != original:
                        print(f"    [Truncate] '{original}' â†’ '{text}' (max 3 decimals)")
        
        elif roi_type == 'INTEGER':
            # 1. æ£€æµ‹å°æ•°ç‚¹ - åªè­¦å‘Š
            if '.' in text:
                print(f"    âš ï¸ [Warning] Decimal in INTEGER: '{text}' - keeping for review")
            
            # 2. æ£€æµ‹å¯èƒ½çš„é‡å¤æ¨¡å¼ - åªè­¦å‘Šï¼Œä¸è‡ªåŠ¨ä¿®å¤
            clean_text = re.sub(r'[^\d-]', '', text)
            if clean_text:
                length = len(clean_text.lstrip('-'))
                for repeat_len in range(1, length // 2 + 1):
                    base = clean_text[:repeat_len + (1 if clean_text.startswith('-') else 0)]
                    if clean_text.startswith('-'):
                        pattern = base + base[1:] * ((length // repeat_len) - 1)
                    else:
                        pattern = base * (length // repeat_len)
                    if pattern == clean_text and length >= repeat_len * 2:
                        print(f"    âš ï¸ [Warning] Possible repeat pattern: '{text}' - keeping as-is for review")
                        break
        
        return text
    
    def find_crop_image(self, csv_base, filename, roi_id):
        """æŸ¥æ‰¾è£å‰ªå›¾åƒ - æ”¯æŒå¤šè·¯å¾„å›é€€"""
        folder_name = os.path.splitext(filename)[0]
        
        # Primary: crops_base
        potential_paths = [
            self.crops_base / csv_base / folder_name / f"{roi_id}.jpg",
            self.crops_base / csv_base / folder_name / f"{roi_id}.png",
            self.crops_base / folder_name / f"{roi_id}.jpg",
            self.crops_base / folder_name / f"{roi_id}.png",
        ]
        
        # Fallback 1: DEBUG_CROPS_INPUT (flattened)
        potential_paths.extend([
            DEBUG_CROPS_INPUT / folder_name / f"{roi_id}.jpg",
            DEBUG_CROPS_INPUT / folder_name / f"{roi_id}.png",
        ])
        
        # Fallback 2: DEBUG_CROPS_BASE (if different from crops_base)
        if DEBUG_CROPS_BASE != self.crops_base:
            potential_paths.extend([
                DEBUG_CROPS_BASE / folder_name / f"{roi_id}.jpg",
                DEBUG_CROPS_BASE / folder_name / f"{roi_id}.png",
                DEBUG_CROPS_BASE / csv_base / folder_name / f"{roi_id}.jpg",
                DEBUG_CROPS_BASE / csv_base / folder_name / f"{roi_id}.png",
            ])
        
        # Fallback 3: MANUAL_CHECK paths (Abnormal)
        potential_paths.extend([
            MANUAL_CHECK_BASE_Abnormal / csv_base / folder_name / f"{roi_id}.jpg",
            MANUAL_CHECK_BASE_Abnormal / csv_base / folder_name / f"{roi_id}.png",
            MANUAL_CHECK_BASE_Abnormal / folder_name / f"{roi_id}.jpg",
            MANUAL_CHECK_BASE_Abnormal / folder_name / f"{roi_id}.png",
        ])
        
        for p in potential_paths:
            if p.exists():
                return p
        return None
    
    def process_abnormal_log(self, log_path, cleaned_csv_path):
        """å¤„ç†å¼‚å¸¸æ—¥å¿—"""
        filename = log_path.name
        csv_base = filename.replace("_Abnormal_Log.csv", "")
        
        print(f"\nğŸ”§ Correcting: {filename}")
        
        try:
            df_bad = pd.read_csv(log_path)
            if df_bad.empty:
                return
            
            # ä»è¾“å…¥CSVåŠ è½½å¹¶è®¡ç®—Medianå€¼
            roi_medians = {}
            if cleaned_csv_path.exists():
                roi_medians = self.calculate_roi_medians(cleaned_csv_path)
            else:
                print(f"  âš ï¸  Cleaned CSV not found, proceeding without median context")
                
        except Exception as e:
            print(f"  âŒ Error: {e}")
            return
        
        df_bad['AI_3B_Corrected'] = ""
        
        for idx, row in df_bad.iterrows():
            roi_id = row['ROI_ID']
            
            # æŸ¥æ‰¾å›¾åƒ
            img_path = self.find_crop_image(csv_base, row['Filename'], roi_id)
            
            if not img_path:
                df_bad.at[idx, 'AI_3B_Corrected'] = "Image Not Found"
                continue
            
            # è·å–median
            curr_median = roi_medians.get(roi_id, None)
            
            # 3Bæ¨ç†
            fixed_val = self.run_3b_inference(img_path, roi_id, curr_median, row['Value'])
            
            print(f"  [{idx+1}/{len(df_bad)}] {roi_id}: {row['Value']} â†’ {fixed_val} (Median: {curr_median})")
            
            df_bad.at[idx, 'AI_3B_Corrected'] = fixed_val
            
            # åŠ¨æ€æ›´æ–°medianï¼ˆåŠ æƒå¹³å‡ï¼‰
            try:
                val_num = float(fixed_val)
                if val_num > 0:
                    if curr_median:
                        roi_medians[roi_id] = (curr_median * 0.9) + (val_num * 0.1)
                    else:
                        roi_medians[roi_id] = val_num
            except:
                pass
        
        # ä¿å­˜
        out_name = filename.replace(".csv", "_AI_3B_Fixed.csv")
        df_bad.to_csv(self.output_dir / out_name, index=False)
        print(f"  âœ… Saved: {out_name}")
    
    def run(self):
        """è¿è¡Œ3Bä¿®æ­£æµç¨‹"""
        print("\n" + "="*60)
        print("STAGE 2: 3B Model Correction")
        print("="*60)
        
        abnormal_logs = list(self.abnormal_logs_dir.glob("*_Abnormal_Log.csv"))
        
        if not abnormal_logs:
            print("âœ… No abnormal logs found - data is clean!")
            return
        
        print(f"Found {len(abnormal_logs)} abnormal logs\n")
        
        for log_path in abnormal_logs:
            base_name = log_path.name.replace("_Abnormal_Log.csv", "")
            cleaned_path = self.cleaned_dir / f"{base_name}_Cleaned.csv"
            
            if cleaned_path.exists():
                self.process_abnormal_log(log_path, cleaned_path)
            else:
                print(f"âš ï¸  Cleaned CSV not found for {base_name}")
        
        print("\nâœ… Stage 2 Complete")

# ================= é˜¶æ®µ3: åˆå¹¶ä¿®æ­£ç»“æœ =================
class Stage3_MergeCorrections:
    """é˜¶æ®µ3: å°†3Bä¿®æ­£ç»“æœåˆå¹¶å›åŸæ•°æ®é›†"""
    
    def __init__(self, cleaned_dir, fixed_logs_dir, output_dir):
        self.cleaned_dir = Path(cleaned_dir)
        self.fixed_logs_dir = Path(fixed_logs_dir)
        self.output_dir = Path(output_dir)
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def merge_single_file(self, fixed_log_path, cleaned_csv_path):
        """åˆå¹¶å•ä¸ªæ–‡ä»¶çš„ä¿®æ­£"""
        filename = fixed_log_path.name
        
        print(f"\nğŸ”€ Merging: {filename}")
        
        try:
            df_fixed = pd.read_csv(fixed_log_path)
            df_original = pd.read_csv(cleaned_csv_path)
        except Exception as e:
            print(f"  âŒ Read error: {e}")
            return
        
        if 'Filename' not in df_fixed.columns or 'ROI_ID' not in df_fixed.columns:
            print(f"  âš ï¸  Missing columns")
            return
        
        update_count = 0
        
        for _, row in df_fixed.iterrows():
            filename_val = row['Filename']
            roi_col = row['ROI_ID']
            new_val = row.get('AI_3B_Corrected', '')
            
            # è·³è¿‡æ— æ•ˆå€¼
            if pd.isna(new_val) or str(new_val).strip() in ["", "Image Not Found", "ERROR"]:
                continue
            
            new_val = str(new_val).strip().replace("'", "").replace('"', '')
            
            if roi_col not in df_original.columns:
                continue
            
            # æ›´æ–°
            match_mask = df_original['Filename'] == filename_val
            if match_mask.any():
                df_original.loc[match_mask, roi_col] = new_val
                update_count += 1
        
        # ä¿å­˜
        base_name = cleaned_csv_path.stem.replace("_Cleaned", "")
        save_path = self.output_dir / f"{base_name}_3B_Corrected.csv"
        df_original.to_csv(save_path, index=False)
        
        print(f"  âœ… Updated {update_count} cells â†’ {save_path.name}")
    
    def run(self):
        """è¿è¡Œåˆå¹¶æµç¨‹"""
        print("\n" + "="*60)
        print("STAGE 3: Merge 3B Corrections")
        print("="*60)
        
        fixed_logs = list(self.fixed_logs_dir.glob("*_AI_3B_Fixed.csv"))
        
        if not fixed_logs:
            print("âœ… No fixed logs to merge")
            return
        
        print(f"Found {len(fixed_logs)} fixed logs\n")
        
        for log_path in fixed_logs:
            base_name = log_path.name.replace("_Abnormal_Log_AI_3B_Fixed.csv", "")
            cleaned_path = self.cleaned_dir / f"{base_name}_Cleaned.csv"
            
            if cleaned_path.exists():
                self.merge_single_file(log_path, cleaned_path)
        
        print("\nâœ… Stage 3 Complete")

# ================= ä¸»æµç¨‹ =================
def main():
    """3Bç®¡é“ä¸»æµç¨‹"""
    print("\n" + "="*80)
    print("ğŸ¤– 3B MODEL DATA CLEANING PIPELINE")
    print("="*80)
    
    # é˜¶æ®µ1: æ•°æ®æ¸…ç†
    stage1 = Stage1_DataCleaning(
        input_dir=STAGE_1_OCR / "CSV_Results",
        output_dir=STAGE_2_CLEANED,
        crops_base=STAGE_1_OCR / "debug_crops"
    )
    stage1.run()
    
    # é˜¶æ®µ2: 3Bä¿®æ­£
    stage2 = Stage2_3BCorrection(
        cleaned_dir=STAGE_2_CLEANED,
        abnormal_logs_dir=STAGE_2_CLEANED,
        crops_base=STAGE_1_OCR / "debug_crops",
        output_dir=STAGE_3_3B_CORRECTED
    )
    stage2.run()
    
    # é˜¶æ®µ3: åˆå¹¶
    stage3 = Stage3_MergeCorrections(
        cleaned_dir=STAGE_2_CLEANED,
        fixed_logs_dir=STAGE_3_3B_CORRECTED,
        output_dir=STAGE_3_3B_CORRECTED
    )
    stage3.run()
    
    print("\n" + "="*80)
    print("ğŸ‰ 3B PIPELINE COMPLETE")
    print(f"ğŸ“‚ Final Output: {STAGE_3_3B_CORRECTED}")
    print("="*80)

if __name__ == "__main__":
    main()

